global:
  otel:
    # -- if enabled, deploys the collector and instruments services - the collector is configured through the opentelemetry-collector section and services through configMapName and secretName
    enabled: true
    # -- url to the opentelemetry collector
    collectorUrl: "release-name-opentelemetry-collector:4317"
    # -- name of the created configmap with extra opentelemetry service configurations
    configMapName: "otel-configmap"
    # -- name of the secret with extra opentelemetry service configurations
    secretName: ""
    # -- enable if you want to deploy jaeger for the collector to send data to
    jaeger: true
    # -- enable if you want to deploy prometheus for the collector to send data to
    prometheus: true

auth:
  licence:
    # -- name of license-key secret, if changed, it must be updated in doc-ver-api
    secretName: "license-key"
    # -- enable if you want to create licence secret as part of this charts deployment
    createSecret: false
    # -- if createSecret is set to true, set the license key here
    licenseKey: ""
  serviceAccount:
    # -- name of the secret, this string value must be updated in both doc-ver-api and embedding-store
    secretName: "sa-json"
    # -- the name of pull secret that will be used to pull images from the registry, if updated, it must be updated in values for all services
    imgPullSecretName: "eu.gcr.io"
    # -- if you want to create service account secret and image pull secret as part of this charts deployment
    createBothSecrets: false
    # -- base64 value of service account json you got from developer.microblink.com
    serviceAccountJsonB64: ""
  dbCreds:
    # -- name of the secret, this string value must be updated in both postgresql and embedding-store
    secretName: "mb-docver-db-creds"
    # -- if you do not expect multiple database users and db will not be exposed to any external traffic, set this to true and it will create secret
    # used by both embedding-store and postgresql (if postgresql is deployed as part of this helm release)
    createSecret: true
    # -- if createSecret is set to true, set the database username here, if you update this value, make sure to update
    # the value in the postgresql section as well (if postgresql is enabled). 
    # if you are using "external db" like cloud SQL or RDS, set this to the username you have created in the database
    username: "embedding-store-sa"
    # -- if createSecret is set to true, set the database password here, we don't expect to have external traffic to the database, so
    # we can use fixed password. If you want to manage user credentials password outside of this helm release simply create a secret
    # with the name you specified under secretName, and disable createSecret. Check out the templates/db-creds.yaml for the 
    # content of the secret
    password: "x9xv1mw0td"

doc-ver-api:
  # -- using fixed number of replicas if autoscaling is not enabled
  replicaCount: 1
  # -- Autoscaling configurations
  autoscaling:
    # -- if enabled, deployment will be autoscaled
    enabled: false
    # -- min replicas hpa will scale down to
    minReplicas: 1
    # -- max replicas hpa will scale up to
    maxReplicas: 3
    memory:
      # -- if enabled, hpa will scale based on memory usage
      enabled: false
      # -- target memory usage percentage
      target: 80
    cpu:
      # -- if enabled, hpa will scale based on cpu usage
      enabled: true
      # -- target cpu usage percentage
      target: 80
  image:
    repository: eu.gcr.io/microblink-identity/web-api-doc-ver
    pullSecrets:
      - name: eu.gcr.io
  env:
    # -- don't change unless communicated by Microblink support team
    LICENSEE: "localhost"
  # -- has to match the name of the secret in auth.licence.secretName, or if you want to 
  # provision secret outside of this chart, has to match the name of the secret. If you are
  # unclear on the content of the secret, check out the tempates/license-key.yaml
  extraSecrets:
    - license-key
  resources:
    # Reducing limits lower then this is not recommended as it will
    # affect the performance of the service.
    limits:
      cpu: 2
      memory: 2Gi
    requests:
      cpu: 1
      memory: 1Gi

  # Define node selector key-value pairs, example:
  # nodeSelector:
  #   kubernetes.io/e2e-az-name: e2e-az1
  #
  # -- deployment node selector
  nodeSelector: {}

  # Define tolerations for pod assignment, example:
  # tolerations:
  # - key: "key"
  #   operator: "Equal"
  #   value: "value"
  #   effect: "NoSchedule"
  #
  # -- deployment tolerations
  tolerations: []
  
  # Define affinity for pod assignment, example:
  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #       - matchExpressions:
  #         - key: kubernetes.io/e2e-az-name
  #           operator: In
  #           values:
  #           - e2e-az1
  #
  # -- deployment affinity
  affinity: {}
  ingress:
    # -- enable if you want to expose the service
    enabled: false
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
      nginx.ingress.kubernetes.io/ssl-redirect: "true"
      nginx.ingress.kubernetes.io/proxy-body-size: 100m
      nginx.ingress.kubernetes.io/client-max-body-size: 50m
      nginx.ingress.kubernetes.io/enable-cors: "true"
      nginx.ingress.kubernetes.io/cors-allow-origin: "*"
      cert-manager.io/cluster-issuer: letsencrypt-production
    tls:
      - secretName: docver-tls
        hosts:
          # -- if you want to expose the service, set the host name
          - host: "docver.microblink.com"
    hosts:
      # -- if you want to expose the service, set the host name
      - host: "docver.microblink.com"
        paths: ["/docver", "/info", "/barcode"]
    pathType: ImplementationSpecific

visual-anomaly:
  # -- using fixed number of replicas if autoscaling is not enabled
  replicaCount: 1
  autoscaling:
    # -- if enabled, deployment will be autoscaled
    enabled: false
    # -- min replicas hpa will scale down to
    minReplicas: 1
    # -- max replicas hpa will scale up to
    maxReplicas: 1
    memory:
      # -- if enabled, hpa will scale based on memory usage
      enabled: false
      # -- target memory usage percentage
      target: 80
    cpu:
      # -- if enabled, hpa will scale based on cpu usage
      enabled: true
      # -- target cpu usage percentage
      target: 80
  image:
    pullSecrets:
      - name: eu.gcr.io
  enabled: true
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 300m
      memory: 0.5Gi
  # -- deployment node selector
  nodeSelector: {}
  # -- deployment tolerations
  tolerations: []
  # -- deployment podAnnotations
  podAnnotations: {}
  # -- deployment affinity
  affinity: {}
  ingress:
    enabled: false

anomdet-intermediary:
  replicaCount: 1
  autoscaling:
    # -- if enabled, deployment will be autoscaled
    enabled: false
    # -- min replicas hpa will scale down to
    minReplicas: 1
    # -- max replicas hpa will scale up to
    maxReplicas: 2
    # -- if set, hpa will scale based on cpu usage, target cpu usage percentage
    targetCPUUtilizationPercentage: 80
    # -- if set, hpa will scale based on memory usage, target memory usage percentage
    targetMemoryUtilizationPercentage: 80
  resources:
    limits:
      # -- deployment resource memory limit
      memory: 1Gi
    requests:
      # -- deployment resource cpu requests
      cpu: 300m
      # -- deployment resource memory requests
      memory: 512Mi
  # -- deployment node selector
  nodeSelector: {}
  # -- deployment tolerations
  tolerations: []
  # -- deployment affinity
  affinity: {}

  # -- do not update anomdetIntermediaryConfig values, they are fixed for a specific docver release
  anomdetIntermediaryConfig:
    model-id: "6478fcb410dcce6d3b037199"
    model-name: "visual-anomaly"
    collection-name: mdv-1519
    # -- each verification request will start ~200 queries, using 10000 to support have max of 50 RPS rate
    # -- without blocking the requests in a sequence
    parallel-queries: 10000
  image:
    repository: eu.gcr.io/microblink-identity/anomaly-detection-intermediary/onprem
    pullSecrets:
      - name: eu.gcr.io
  ingress:
    enabled: false

embedding-store:
  seeder:
    enabled: false
    image:
      repository: "eu.gcr.io/microblink-identity/embedding-store/onprem"
      pullSecrets:
        - name: eu.gcr.io
    grpc:
      grpcRecvSize: "52428800"
      grpcSendSize: "52428800"
    secret: sa-json
    config:
      collectionCreateWorkers: 200
      # -- this controlls the initialization rate of the embedding-store database.
      # -- increasing this value will speed up the initialization process, but it will also increase the load on the database
      # -- make sure the database can handle the load to prevent the database from crashing
      collectionInsertWorkers: 20
      collectionInsertBatch: 1
    seedStore:
      s3:
        enabled: false
      gc:
        enabled: true
        bucket: "docver-va-releases"
        prefix: "full-db-768/6478fcb410dcce6d3b037199"
    resources:
      limits:
        cpu: 1
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi
  server:
    image:
      repository: "eu.gcr.io/microblink-identity/embedding-store/onprem"
      pullSecrets:
        - name: eu.gcr.io
    database:
      pgvector:
        enabled: true
        # If using external database, set this to a valid host and port
        # For example SaaS postgre in AWS or GCP
        addr: "postgresql:5432"
        # -- name of the database, if you are using an external database, set this to the name of the database
        database: postgres
        # -- set this to false if you are using an "external" SaaS database
        addrPrepandReleaseName: true
        connectionStringParams: "pool_max_conns=40&pool_max_conn_idle_time=30s&pool_max_conn_lifetime=60s"
    grpc:
      grpcRecvSize: "52428800"
      grpcSendSize: "52428800"

    autoscaling:
      # -- if enabled, server deployment will be autoscaled
      enabled: false
      # -- min replicas hpa will scale down to
      minReplicas: 1
      # -- max replicas hpa will scale up to
      maxReplicas: 2
      # -- if set, hpa will scale based on cpu usage, target memory usage percentage
      targetCPUUtilizationPercentage: 80
      # -- if set, hpa will scale based on memory usage, target memory usage percentage
      targetMemoryUtilizationPercentage: 80

    # If facking with high RPS load, consider increasing the requirements
    resources:
      limits:
        memory: 2Gi
      requests:
        cpu: 500m
        memory: 1Gi

    # -- server deployment node selector
    nodeSelector: {}
    # -- server deployment tolerations
    tolerations: []
    # -- server deployment affinity
    affinity: {}

    secret: mb-docver-db-creds

postgresql:
  # -- hosting postgres as part of this helm release is disabled by default, if you want to enable it, set enabled to true
  # -- Disabled, as we expect that the database will be hosted outside of this helm release, e.g in AWS RDS or GCP CloudSQL
  enabled: false
  image:
    registry: docker.io
    repository: ankane/pgvector
    tag: v0.5.1
    pullPolicy: IfNotPresent
  global:
    postgresql:
      auth:
        existingSecret: mb-docver-db-creds
        postgresqlDatabase: postgres
  auth:
    # -- must be fixed to this value, do not change
    username: embedding-store-sa
  # volumePermissions:
    # enabled: true
  primary:
    args:
      - -c
      - config_file=/bitnami/postgresql/conf/postgresql.conf
    persistence:
      enabled: true
      ## -- change the storeage class to the one that is available in your kubernetes cluster
      storageClass: "default"
      size: 500Gi
    service:
      type: ClusterIP
    resources:
      limits:
        memory: "24Gi"
      requests:
        cpu: 6
        memory: "20Gi"
    nodeSelector:
      workload: postgre-db
    tolerations:
      - key: "kubernetes.io/workload"
        operator: "Equal"
        value: "storage"
        effect: "NoSchedule"
      - key: "kubernetes.io/workload"
        operator: "Equal"
        value: "postgre-db"
        effect: "NoSchedule"
    # Patch to prevent postgre chart bug
    affinity:
      podAntiAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: postgresql
                  app.kubernetes.io/instance: argo-wf
                  app.kubernetes.io/component: primary
              namespaces:
                - "placeholder"
              topologyKey: kubernetes.io/hostname
            weight: 1
    livenessProbe:
      failureThreshold: 600
    configuration: |
      max_connections = 1000
      shared_buffers = 18GB
      effective_cache_size = 24GB
      maintenance_work_mem = 1GB
      checkpoint_completion_target = 0.9
      wal_buffers = 16MB
      default_statistics_target = 1000
      random_page_cost = 1.1
      effective_io_concurrency = 300
      work_mem = 2MB
      huge_pages = off
      min_wal_size = 2GB
      max_wal_size = 4GB
      max_worker_processes = 10
      max_parallel_workers_per_gather = 4
      max_parallel_workers = 10
      max_parallel_maintenance_workers = 4

      wal_level = minimal
      max_wal_senders = 0

      listen_addresses = '*'

mlp-local-storage:
  # -- enable this ONLY if you do not have dynamic storage provisioning in k8s cluster, likely using on-prem, baremetal k8s
  enabled: false
  StorageClass:
    - create: true
      name: microblink-docver
      provisioner: kubernetes.io/no-provisioner
  PersistentVolume:
    - volumenameprefix: release-name-
      storageClass:
        name: microblink-docver
        reclaimPolicy: Delete
      owner: 1001
      capacity: 1300Gi
      storageType: ssd
      nodes:
        - s1

bundle-visual-anomaly-core-versions:
  bundle:
    serving:
      nginx:
        resolver: "kube-dns.kube-system.svc.cluster.local"
    proxy:
      image:
        repository: eu.gcr.io/microblink-identity/mlp-model-proxy/onprem
        tag: v0.17.7
      minLimits:
        cpu: 500m
        memory: "1Gi"
      maxLimits:
        memory: "2Gi"
      ingress:
        enabled: false
      autoscaling:
        enabled: true
        minReplicas: 1
        maxReplicas: 3
        targetCPUUtilizationPercentage: "80"
      env:
        GOGC: "50"
    models:
      autoscaling:
        type: hpa
        minReplicas: 1
        maxReplicas: 3
      model:
        globalStorage:
          type: gs
          secret: sa-json
          bucket: identity-enc-models-prod
        storage:
          type: gs
          secret: sa-json
          bucket: identity-enc-models-prod
  models:
    6478fcb410dcce6d3b037199:
      # Model resources can be reduced, however, it is not recommended if
      # the model is used in production.
      minLimits:
        cpu: 2
        memory: "2Gi"
      maxLimits:
        memory: "2Gi"
      engine:
        type: "triton"
      image:
        repository: "eu.gcr.io/microblink-identity/tritonserver-cpu-onnxruntime/onprem"
        tag: "23.06"

# this will deploy the opentelemetry collector as a statefulset next to the application
opentelemetry-collector:
  mode: statefulset
  tolerations: []

  config:
    # -- monitoring tools should be added beforehand and the exporters section should be configured accordingly
    # -- jaeger/zipkin/prometheus/datadog/...
    exporters:
      prometheus:
        endpoint: 0.0.0.0:8889
      otlp:
        endpoint: "release-name-jaeger-collector:4317"
        tls:
          insecure: true

    # -- default receivers for the collector, our services support only the grpc receiver
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317

    # -- Processors documentation:  https://opentelemetry.io/docs/collector/configuration/#processors
    processors:
      batch:
        send_batch_size: 50000
        timeout: 3s
      # -- add custom extra attributes to the traces
      attributes:
        actions:
          key: deployment.environment
          value: dev
          action: insert
      # -- trigger the garbage collector when the memory limit is reached, so that the collector doesn't go out of memory
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 50
      # -- sample 10% of all traces and all traces with status code ERROR/UNSET
      tail_sampling:
        # -- time to wait before a decision is made
        decision_wait: 10s
        # -- the number of traces to process in the tail sampler
        num_traces: 100
        # -- the number of traces you're expected to receive per second
        # -- it should be modified accordingly to the expected load so that the processor can work better
        expected_new_traces_per_sec: 10
        # -- other policies can be added here: https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor
        policies:
          - name: trace-10-percent-of-all
            type: probabilistic
            probabilistic: { sampling_percentage: 10 }
          - name: trace-errors
            type: status_code
            status_code: { status_codes: [ERROR, UNSET] }

    # -- Connector documentation https://opentelemetry.io/docs/collector/configuration/#connectors
    connectors:
      # -- this will calculate latency metrics for all traces and provide it as a histogram for visualization
      spanmetrics:
        histogram:
          explicit:
            buckets: [100us, 1ms, 2ms, 6ms, 10ms, 100ms, 250ms]
        aggregation_temporality: "AGGREGATION_TEMPORALITY_CUMULATIVE"

    # -- Extensions documentation https://opentelemetry.io/docs/collector/configuration/#extensions
    extensions:
      health_check:
        endpoint: "0.0.0.0:13133"
      zpages:
        endpoint: localhost:1777
      pprof:
        endpoint: localhost:55679

    # -- Defines the entire pipeline of the collector
    service:
      extensions: [health_check, zpages, pprof]
      pipelines:
        traces:
          receivers: [otlp]
          processors: [attributes, memory_limiter, tail_sampling, batch]
          exporters: [logging, spanmetrics, otlp]
        metrics:
          receivers: [otlp, spanmetrics]
          processors: [attributes, memory_limiter, batch]
          exporters: [logging, prometheus]

# -- the following section is an example deployment of monitoring tools where the traces and metrics can be viewed
# -- in a production environment, the monitoring tools should be deployed separately and only configured as exporters in the opentelemetry collector
jaeger:
  provisionDataStore:
    cassandra: false
  allInOne:
    nodeSelector:
      gpu-accelerator-type: none
    enabled: true
    resources:
      limits:
        memory: 1Gi
      requests:
        cpu: 1
        memory: 1Gi
  storage:
    type: none
  agent:
    enabled: false
  collector:
    enabled: false
  query:
    enabled: false

prometheus:
  rbac:
    create: false
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false
  prometheus-pushgateway:
    enabled: false
  prometheus.yml:
    scrape_configs:
      - job_name: "otel-collector"
        scrape_interval: 10s
        static_configs:
          - targets: ["release-name-opentelemetry-collector:8889"]
  server:
    resources:
      limits:
        memory: 1Gi
      requests:
        cpu: 1
        memory: 1Gi
    tolerations: []
    affinity: {}
    persistentVolume:
      enabled: false
